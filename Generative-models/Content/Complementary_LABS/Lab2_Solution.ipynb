{"cells":[{"cell_type":"markdown","source":["# Lab 2: CGANs on CIFAR-10"],"metadata":{"id":"i8s1pBBoV7cy"},"id":"i8s1pBBoV7cy"},{"cell_type":"code","execution_count":1,"id":"d6f1ff15-5e18-4950-b313-d11660c446ed","metadata":{"id":"d6f1ff15-5e18-4950-b313-d11660c446ed","executionInfo":{"status":"ok","timestamp":1658238776036,"user_tz":-330,"elapsed":2154,"user":{"displayName":"Arya Mohan","userId":"15606194433450766788"}}},"outputs":[],"source":["from __future__ import print_function, division\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Reshape, Concatenate\n","from tensorflow.keras.layers import BatchNormalization, Activation, Conv2D, Conv2DTranspose, LeakyReLU, ReLU, Embedding\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.datasets import cifar10, mnist\n","from keras.preprocessing import image\n","import keras.backend as K\n","import matplotlib.pyplot as plt\n","import sys\n","import numpy as np\n","import time\n","from tqdm import tqdm\n","\n","%matplotlib inline"]},{"cell_type":"markdown","id":"a17f97e3-9bc5-4781-b202-991c1ee86c64","metadata":{"tags":[],"id":"a17f97e3-9bc5-4781-b202-991c1ee86c64"},"source":["## Parameters"]},{"cell_type":"code","execution_count":null,"id":"aef687ab-ccfd-416f-96e2-6a542344ce05","metadata":{"id":"aef687ab-ccfd-416f-96e2-6a542344ce05"},"outputs":[],"source":["BATCH_SIZE = 16\n","EPOCHS = 100\n","\n","NOISE_DIM = 100 \n","NUM_CLASSES = 10\n","tags = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n","\n","IMG_SIZE = 32"]},{"cell_type":"code","execution_count":null,"id":"3c15320b-4473-46c8-9771-c925aa093f07","metadata":{"id":"3c15320b-4473-46c8-9771-c925aa093f07"},"outputs":[],"source":["# Helper function to plot generated images\n","def show_samples(num_samples, NUM_CLASSES, g_model):\n","    fig, axes = plt.subplots(10,num_samples, figsize=(10,20)) \n","    fig.tight_layout()\n","    fig.subplots_adjust(wspace=None, hspace=0.2)\n","\n","    for l in np.arange(10):\n","      random_noise = tf.random.normal(shape=(num_samples, NOISE_DIM))\n","      label = tf.ones(num_samples)*l\n","      gen_imgs = g_model.predict([random_noise, label])\n","      for j in range(gen_imgs.shape[0]):\n","        img = image.array_to_img(gen_imgs[j], scale=True)\n","        axes[l,j].imshow(img)\n","        axes[l,j].yaxis.set_ticks([])\n","        axes[l,j].xaxis.set_ticks([])\n","\n","        if j ==0:\n","          axes[l,j].set_ylabel(tags[l])\n","    plt.show()"]},{"cell_type":"markdown","id":"5bcfc255-f936-4fb0-a6dc-a13997795942","metadata":{"tags":[],"id":"5bcfc255-f936-4fb0-a6dc-a13997795942"},"source":["## Loading Data"]},{"cell_type":"code","execution_count":null,"id":"5013f612-1f99-40df-a514-b60371f6e787","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"5013f612-1f99-40df-a514-b60371f6e787","executionInfo":{"status":"ok","timestamp":1656823755753,"user_tz":-330,"elapsed":2646,"user":{"displayName":"Arya Mohan","userId":"15606194433450766788"}},"outputId":"a4ebcecc-07a8-45d7-af62-915e3f0fdc76"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train Shape: ((50000, 32, 32, 3), (50000, 1))\n"]}],"source":["(X_train, y_train), (_, _) = cifar10.load_data()\n","\n","# Normalize data\n","X_train = (X_train - 127.5) / 127.5\n","print (f\"Train Shape: ({X_train.shape}, {y_train.shape})\")\n"," \n","# Create tf.data.Dataset\n","dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n","dataset = dataset.shuffle(buffer_size=1000).batch(BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"id":"22ae2acc-452a-4c17-9192-bcc5fa2fb8ad","metadata":{"id":"22ae2acc-452a-4c17-9192-bcc5fa2fb8ad","colab":{"base_uri":"https://localhost:8080/","height":174},"executionInfo":{"status":"ok","timestamp":1656823755753,"user_tz":-330,"elapsed":4,"user":{"displayName":"Arya Mohan","userId":"15606194433450766788"}},"outputId":"613ff3f4-8ec7-4957-9aec-b3531e3b96f3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 1.0, 'Bird')"]},"metadata":{},"execution_count":31},{"output_type":"display_data","data":{"text/plain":["<Figure size 144x144 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["plt.figure(figsize=(2,2))\n","idx = np.random.randint(0,len(X_train))\n","img = image.array_to_img(X_train[idx], scale=True)\n","plt.imshow(img)\n","plt.axis('off')\n","plt.title(tags[y_train[idx][0]])"]},{"cell_type":"markdown","id":"c7bcdfe9-5bc0-4b5d-9ee0-0732d31f3b8e","metadata":{"tags":[],"id":"c7bcdfe9-5bc0-4b5d-9ee0-0732d31f3b8e"},"source":["## Loss and Optimizers"]},{"cell_type":"code","execution_count":null,"id":"18ce7580-6ddf-4f9c-ba18-ac705b6fc3e0","metadata":{"id":"18ce7580-6ddf-4f9c-ba18-ac705b6fc3e0"},"outputs":[],"source":["bce_loss = tf.keras.losses.BinaryCrossentropy()"]},{"cell_type":"code","execution_count":null,"id":"9638afd4-a5c7-4637-884f-b69222786908","metadata":{"id":"9638afd4-a5c7-4637-884f-b69222786908"},"outputs":[],"source":["# Discriminator Loss\n","def discriminator_loss(real_output, fake_output):\n","    real_loss = bce_loss(tf.ones_like(real_output), real_output)\n","    fake_loss = bce_loss(tf.zeros_like(fake_output), fake_output)\n","    total_loss = real_loss + fake_loss\n","    return total_loss"]},{"cell_type":"code","execution_count":null,"id":"35173412-e720-4a32-a99c-349caab1baa9","metadata":{"id":"35173412-e720-4a32-a99c-349caab1baa9"},"outputs":[],"source":["def generator_loss(d_predictions):\n","    return bce_loss(tf.ones_like(d_predictions), d_predictions)"]},{"cell_type":"code","execution_count":null,"id":"49fe2ef1-a122-4317-8c77-c31731d9f46a","metadata":{"id":"49fe2ef1-a122-4317-8c77-c31731d9f46a"},"outputs":[],"source":["d_optimizer=Adam(learning_rate=0.0002, beta_1 = 0.5)\n","g_optimizer=Adam(learning_rate=0.0002, beta_1 = 0.5)"]},{"cell_type":"markdown","id":"bc52b093-eee9-4520-baa4-72a23431ce5a","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"bc52b093-eee9-4520-baa4-72a23431ce5a"},"source":["## Build Generator and Discriminator Architecture"]},{"cell_type":"code","execution_count":null,"id":"113980aa-b837-4596-abeb-54d2beb54ab2","metadata":{"id":"113980aa-b837-4596-abeb-54d2beb54ab2"},"outputs":[],"source":["def build_generator():\n","    \n","  # label input\n","\tin_label = Input(shape=(1,))  #Input of dimension 1\n","\t# embedding for categorical input\n","  # each label (total 10 classes for cifar), will be represented by a vector of size 50. \n","\tli = Embedding(NUM_CLASSES, 50)(in_label) #Shape 1,50\n","    \n","\t# linear multiplication\n","\tn_nodes = 8 * 8  # To match the dimensions for concatenation later in this step.  \n","\tli = Dense(n_nodes)(li) #1,64\n","\t# reshape to additional channel\n","\tli = Reshape((8, 8, 1))(li)\n","    \n","    \n","\t# image generator input\n","\tin_lat = Input(shape=(NOISE_DIM,))  #Input of dimension 100\n","\n","\tn_nodes = 128 * 8 * 8\n","\tgen = Dense(n_nodes)(in_lat)  #shape=8192\n","\tgen = LeakyReLU(alpha=0.2)(gen)\n","\tgen = Reshape((8, 8, 128))(gen) #Shape=8x8x128\n","\t# merge image gen and label input\n","\tmerge = Concatenate()([gen, li])  #Shape=8x8x129 (Extra channel corresponds to the label)\n","\t# upsample to 16x16\n","\tgen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge) #16x16x128\n","\tgen = LeakyReLU(alpha=0.2)(gen)\n","\t# upsample to 32x32\n","\tgen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen) #32x32x128\n","\tgen = LeakyReLU(alpha=0.2)(gen)\n","\t# output\n","\tout_layer = Conv2D(3, (8,8), activation='tanh', padding='same')(gen) #32x32x3\n","\t# define model\n","\tmodel = Model([in_lat, in_label], out_layer)\n","\treturn model   #Model not compiled as it is not directly trained like the discriminator."]},{"cell_type":"code","execution_count":null,"id":"bdf4d0ac-455f-463a-96d4-ed1eccdefbb0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bdf4d0ac-455f-463a-96d4-ed1eccdefbb0","executionInfo":{"status":"ok","timestamp":1656823756363,"user_tz":-330,"elapsed":26,"user":{"displayName":"Arya Mohan","userId":"15606194433450766788"}},"outputId":"4e3bb0e4-54b5-40e1-b93f-16d08c5b4f32"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_2\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_6 (InputLayer)           [(None, 100)]        0           []                               \n","                                                                                                  \n"," input_5 (InputLayer)           [(None, 1)]          0           []                               \n","                                                                                                  \n"," dense_5 (Dense)                (None, 8192)         827392      ['input_6[0][0]']                \n","                                                                                                  \n"," embedding_2 (Embedding)        (None, 1, 50)        500         ['input_5[0][0]']                \n","                                                                                                  \n"," leaky_re_lu_5 (LeakyReLU)      (None, 8192)         0           ['dense_5[0][0]']                \n","                                                                                                  \n"," dense_4 (Dense)                (None, 1, 64)        3264        ['embedding_2[0][0]']            \n","                                                                                                  \n"," reshape_4 (Reshape)            (None, 8, 8, 128)    0           ['leaky_re_lu_5[0][0]']          \n","                                                                                                  \n"," reshape_3 (Reshape)            (None, 8, 8, 1)      0           ['dense_4[0][0]']                \n","                                                                                                  \n"," concatenate_2 (Concatenate)    (None, 8, 8, 129)    0           ['reshape_4[0][0]',              \n","                                                                  'reshape_3[0][0]']              \n","                                                                                                  \n"," conv2d_transpose_2 (Conv2DTran  (None, 16, 16, 128)  264320     ['concatenate_2[0][0]']          \n"," spose)                                                                                           \n","                                                                                                  \n"," leaky_re_lu_6 (LeakyReLU)      (None, 16, 16, 128)  0           ['conv2d_transpose_2[0][0]']     \n","                                                                                                  \n"," conv2d_transpose_3 (Conv2DTran  (None, 32, 32, 128)  262272     ['leaky_re_lu_6[0][0]']          \n"," spose)                                                                                           \n","                                                                                                  \n"," leaky_re_lu_7 (LeakyReLU)      (None, 32, 32, 128)  0           ['conv2d_transpose_3[0][0]']     \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 32, 32, 3)    24579       ['leaky_re_lu_7[0][0]']          \n","                                                                                                  \n","==================================================================================================\n","Total params: 1,382,327\n","Trainable params: 1,382,327\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["g_model = build_generator()\n","g_model.summary()"]},{"cell_type":"code","execution_count":null,"id":"ffa7ea4d-b7a7-47dd-a64f-4e25a4862988","metadata":{"id":"ffa7ea4d-b7a7-47dd-a64f-4e25a4862988"},"outputs":[],"source":["def build_discriminator():\n","    \n","  # label input\n","  in_label = Input(shape=(1,))  #Shape 1\n","  # embedding for categorical input\n","  #each label (total 10 classes for cifar), will be represented by a vector of size 50. \n","  #This vector of size 50 will be learnt by the discriminator\n","  li = Embedding(NUM_CLASSES, 50)(in_label) #Shape 1,50\n","  # scale up to image dimensions with linear activation\n","  n_nodes = IMG_SIZE * IMG_SIZE  #32x32 = 1024. \n","  li = Dense(n_nodes)(li)  #Shape = 1, 1024\n","  # reshape to additional channel\n","  li = Reshape((IMG_SIZE, IMG_SIZE, 1))(li)  #32x32x1\n","\n","\n","  # image input\n","  in_image = Input(shape=(IMG_SIZE, IMG_SIZE, 3)) #32x32x3\n","  # concat label as a channel\n","  merge = Concatenate()([in_image, li]) #32x32x4 (4 channels, 3 for image and the other for labels)\n","\n","  # downsample: This part is same as unconditional GAN upto the output layer.\n","  #We will combine input label with input image and supply as inputs to the model. \n","  fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(merge) #16x16x128\n","  fe = LeakyReLU(alpha=0.2)(fe)\n","  # downsample\n","  fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe) #8x8x128\n","  fe = LeakyReLU(alpha=0.2)(fe)\n","  # flatten feature maps\n","  fe = Flatten()(fe)  #8192  (8*8*128=8192)\n","  # dropout\n","  fe = Dropout(0.4)(fe)\n","  # output\n","  out_layer = Dense(1, activation='sigmoid')(fe)  #Shape=1\n","\n","  # define model\n","  ##Combine input label with input image and supply as inputs to the model. \n","  model = Model([in_image, in_label], out_layer)\n","      \n","  return model"]},{"cell_type":"code","execution_count":null,"id":"ca19a7ed-b310-4b73-a1a6-88d0810e84e2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ca19a7ed-b310-4b73-a1a6-88d0810e84e2","executionInfo":{"status":"ok","timestamp":1656823756363,"user_tz":-330,"elapsed":17,"user":{"displayName":"Arya Mohan","userId":"15606194433450766788"}},"outputId":"99ce1946-cc4a-484b-fc63-75a5e8b5ccdc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_3\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_7 (InputLayer)           [(None, 1)]          0           []                               \n","                                                                                                  \n"," embedding_3 (Embedding)        (None, 1, 50)        500         ['input_7[0][0]']                \n","                                                                                                  \n"," dense_6 (Dense)                (None, 1, 1024)      52224       ['embedding_3[0][0]']            \n","                                                                                                  \n"," input_8 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n","                                                                                                  \n"," reshape_5 (Reshape)            (None, 32, 32, 1)    0           ['dense_6[0][0]']                \n","                                                                                                  \n"," concatenate_3 (Concatenate)    (None, 32, 32, 4)    0           ['input_8[0][0]',                \n","                                                                  'reshape_5[0][0]']              \n","                                                                                                  \n"," conv2d_4 (Conv2D)              (None, 16, 16, 128)  4736        ['concatenate_3[0][0]']          \n","                                                                                                  \n"," leaky_re_lu_8 (LeakyReLU)      (None, 16, 16, 128)  0           ['conv2d_4[0][0]']               \n","                                                                                                  \n"," conv2d_5 (Conv2D)              (None, 8, 8, 128)    147584      ['leaky_re_lu_8[0][0]']          \n","                                                                                                  \n"," leaky_re_lu_9 (LeakyReLU)      (None, 8, 8, 128)    0           ['conv2d_5[0][0]']               \n","                                                                                                  \n"," flatten_1 (Flatten)            (None, 8192)         0           ['leaky_re_lu_9[0][0]']          \n","                                                                                                  \n"," dropout_1 (Dropout)            (None, 8192)         0           ['flatten_1[0][0]']              \n","                                                                                                  \n"," dense_7 (Dense)                (None, 1)            8193        ['dropout_1[0][0]']              \n","                                                                                                  \n","==================================================================================================\n","Total params: 213,237\n","Trainable params: 213,237\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["d_model = build_discriminator()\n","d_model.summary()"]},{"cell_type":"markdown","id":"ea20ae46-90bd-4678-b5df-b398e1a61d43","metadata":{"id":"ea20ae46-90bd-4678-b5df-b398e1a61d43"},"source":["## Train the GAN"]},{"cell_type":"code","execution_count":null,"id":"72cd9996-70ff-4ecd-9b8d-2381a6acf732","metadata":{"id":"72cd9996-70ff-4ecd-9b8d-2381a6acf732"},"outputs":[],"source":["# Compiles the train_step function into a callable TensorFlow graph\n","# Also speeds up the training time\n","@tf.function\n","def train_step(dataset):\n","   \n","    real_images, real_labels = dataset\n","    # Sample random points in the latent space and concatenate the labels.\n","    random_latent_vectors = tf.random.normal(shape=(BATCH_SIZE, NOISE_DIM))\n","    generated_images = g_model([random_latent_vectors, real_labels])\n","    # print(np.amax(generated_images.numpy()), np.amin(generated_images.numpy()))\n","\n","    # Train the discriminator.\n","    with tf.GradientTape() as tape:\n","        pred_fake = d_model([generated_images, real_labels])\n","        pred_real = d_model([real_images, real_labels])\n","        \n","        d_loss = discriminator_loss(pred_real, pred_fake)\n","      \n","    grads = tape.gradient(d_loss, d_model.trainable_variables)\n","    # print(grads)\n","    d_optimizer.apply_gradients(zip(grads, d_model.trainable_variables))\n","\n","    #-----------------------------------------------------------------#\n","    \n","    # Sample random points in the latent space.\n","    random_latent_vectors = tf.random.normal(shape=(BATCH_SIZE, NOISE_DIM))\n","   \n","    # Train the generator (note that we should *not* update the weights\n","    # of the discriminator)!\n","    with tf.GradientTape() as tape:\n","        fake_images = g_model([random_latent_vectors, real_labels])\n","        # print(np.amax(fake_images.numpy()), np.amin(fake_images.numpy()))\n","        predictions = d_model([fake_images, real_labels])\n","        g_loss = generator_loss(predictions)\n","    \n","    grads = tape.gradient(g_loss, g_model.trainable_variables)\n","    # print(grads)\n","    g_optimizer.apply_gradients(zip(grads, g_model.trainable_variables))\n","    \n","    return d_loss, g_loss"]},{"cell_type":"code","execution_count":null,"id":"3e840ad8-8e21-4fbf-b9dc-8f100618ec53","metadata":{"id":"3e840ad8-8e21-4fbf-b9dc-8f100618ec53"},"outputs":[],"source":["def train(dataset, epochs=EPOCHS):\n","\n","    for epoch in range(EPOCHS):\n","        print('Epoch: ', epoch)\n","        d_loss_list = []\n","        g_loss_list = []\n","        q_loss_list = []\n","        start = time.time()\n","        \n","        itern = 0\n","        for image_batch in tqdm(dataset):\n","            d_loss, g_loss = train_step(image_batch)\n","            d_loss_list.append(d_loss)\n","            g_loss_list.append(g_loss)\n","            itern=itern+1\n","                \n","        show_samples(5, NUM_CLASSES, g_model)\n","            \n","        print (f'Epoch: {epoch} -- Generator Loss: {np.mean(g_loss_list)}, Discriminator Loss: {np.mean(d_loss_list)}\\n')\n","        print (f'Took {time.time()-start} seconds. \\n\\n')"]},{"cell_type":"code","execution_count":null,"id":"055facb8-8cd2-4b8d-809d-60e065b35e1c","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Pg13zlNS1BWuKGmozSlYi9HD5RQre8kD"},"id":"055facb8-8cd2-4b8d-809d-60e065b35e1c","outputId":"80ac9312-12af-417a-9c57-625f2087be5d"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["train(dataset, epochs=EPOCHS)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"name":"Lab2_Solution.ipynb","provenance":[],"collapsed_sections":["c7bcdfe9-5bc0-4b5d-9ee0-0732d31f3b8e","bc52b093-eee9-4520-baa4-72a23431ce5a"]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}